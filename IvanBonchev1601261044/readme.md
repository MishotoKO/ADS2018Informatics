Aлгоритъма за сортиране сортиране чрез пряка селекция (selection sort) и сортиране чрез вмъкване (insertion sort), имат най-лошо време за изпълнение \Theta(n^2) ?(n 2 ). Когато размерът на входния масив е голям, изпълнението на тези алгоритми може да отнеме много време. В този и в следващия урок ще разгледаме други два алгоритъма за сортиране, сортиране чрез сливане (merge sort) и бързо сортиране (quick sort), които имат по-добро време за изпълнение. По-конкретно, сортиране чрез сливане се изпълнява за време \Theta(n \lg n) ?(nlgn) във всички случаи, а бързото сортиране се изпълнява за време \Theta(n \lg n) ?(nlgn) в най-добрия случай, а времето за изпълнение в най-лошия случай е \Theta(n^2) ?(n ). На таблицата са представени четирите алгоритъма със съответното им време за изпълнение:

Разделяй и владей И двата алгоритъма споделят обща алгоритмична парадигма, базирана на рекурсия. Тази парадигма, наречена разделяй и владей разделя задачата на подзадачи, които са подобни на първоначалната задача, решава ги рекурсивно и накрая комбинира решението, за да реши първоначалната задача. Тъй като "разделяй и владей" решава подзадачите рекурсивно, всяка подзадача трябва да е по-малка от първоначалната задача и трябва да има базов случай за всяка подзадача. Алгоритъмът "разделяй и владей" може да се раздели на 3 части: Разделя задачата на подзадачи, които са по-малки инстанции на същата задача. Овладява подзадачите, като ги решава рекурсивно. Ако са достатъчно малки, решава подзадачите като базови случаи. Комбинира решенията на подзадачите в решение на първоначалната задача.

1 Cортиране на Хоор

Бързо сортиране (на английски: quick sort) е добре известен сортиращ алгоритъм, разработен от Ч. А. Р. Хор през 1960 г.[1] и публикуван през 1961 г.,[2]. Основната част на алгоритъма се състои в сравняващо сортиране.

В най-лошия случай алгоритъмът има сложност O(n2). Средната времева сложност е O(n log n), а амортизираната сложност е 1,386 n log n сравнения. В практиката бързото сортиране е с по-добро време от други сортиращи алгоритми с време O(n log n). Освен това, последователността на бързото сортиране и локализираните препратки към паметта работят добре с кеша. Бързото сортиране се основава на сравнения. То не е устойчиво, тоест може да размества елементи с еднакви ключове. Класическият алгоритъм използва допълнителен масив, но съществува вариант, който сортира данните на място – без заделяне на втори масив, така че се използва само O(log n) допълнителна памет.

2 Пирамидално сортиране

Пирамидално сортиране (на английски: Heapsort) е детермистичен алгоритъм за сортиране, който създава сортиран масив (или списък). Heapsort е вид алгоритъм за сортиране чрез пряка селекция. Въпреки че е по-бавен, със сложност O (n log n), отколкото едно добро приложение на quicksort алгоритъм, той има предимството за по-благоприятен най-лош случай, защото не използва много масиви или рекурсия. Heapsort е измислен от J. W. J. Williams през 1964 г.

3 Побитово сортиране Елементите се групират в "контейнери" Елементите във всеки един "контейнер" се сортират Bucket sort, или bin sort, е сортиращ алгоритъм, който работи чрез разделяне на един масив в определен брой контейнери. След това всеки "контейнер" се сортира индивидуално, или чрез използване на различни сортиращи алгоритми, или чрез рекурсивно прилагане на bucket сортиране. Bucket сортиране е обобщаващ на pigeonhole sort, алгоритъм. Пресмятането на изчислителната сложност включва броя на използваните "контейнери". Bucket sort може да се изпълнява с линейно време – (?(n)). Всяка кофа трябва да съдържа или един елемент, или да се сортира.

Bucket sort работи по следния начин: Създава масив от празни "контейнери". Разделя: Минава през елементите на оригиналния масив, слагайки всеки елемент в "контейнер". Сортира "контейнер", който не е празен. Събира: Минава през всеки един от "контейнерите" и връща елементите в оригиналния масив.

4 Двоично търсене

Методът двоично търсене се нарича Binary Search на английски. В литературата и в университета също така може да се срещне като '''Bisection Method''' (метод на бисекцията) или Dichotomy (Дихотомия).

В информатиката, двоично търсене е алгоритъм, който се използва за намиране на позицията на елемент или стойност в подредена структура от данни, например в предварително сортиран масив. За разлика от последователното търсене, при двоичното търсене са необходими много по-малко стъпки за намиране на търсения елемент. При двоичното търсене, размерът на претърсваното пространство намалява на половина след всяка стъпка. Съществува също така модификация на техниката, при която той намалява само с една трета, но пък решава по-сложен проблем. Тази техника се нарича троично търсене.

Когато искаме многократно да търсим различни елементи в даден масив, е по-добре първо да го сортираме и после да използваме метода на двоичното търсене. Това е бърз метод за претърсване на вече сортиран масив. Двоичното търсене се основава на разделянето на дадено множество от записи на две равни части, сравняване на търсения идентификатор с последния запис от долната половина или с първия запис от горната половина и установяване по този начин в коя половина се намира той. Следва търсене в така откритата половина чрез нейното разделяне на две части и така нататък докато се стигне до желания запис. По този начин се намалява броят на четенията на записи и се спестява време от тази най-времеемка компютърна операция. Според теория на вероятностите при брой на записите n и последователно търсене средновероятният брой на четенията и свързаните с тях сравнения е n/2. При двоично търсене този брой е значително по-малък. Тъй като, само по себе си, двоичното търсене е сравнително кратко и просто за писане, най-често в задачи то бива съчетано с друг, по-сложен алгоритъм или структура данни. Много често се срещат не малък брой задачи, които се решават (поне частично) с двоично търсене. Този метод е много важен и по друга причина – той е един от популярните въпроси на интервюта за работа. Редица софтуерни компании считат, че ако кандидатът за работно място не може да напише двоично търсене, то няма никакъв смисъл да го наемат като програмист.

Максималният брой стъпки, необходими на алгоритъма за намирането на елемент в сортиран масив, е двоичен логаритъм от дължината на масива. Тоест, за намиране на елемент в сортиран масив от 1 милион елемента алгоритъмът ще извърши най-много 20 стъпки. На всяка стъпка алгоритъмът сравнява търсената стойност със стойността на средния елемент от масива. В случай че средният елемент съвпада с търсения елемент, тогава алгоритъмът завършва – елементът е намерен и неговият индекс (позиция) се връща.

5 Фибоначи Числата на Фибоначи в математиката образуват редица, която се дефинира рекурсивно по следния начин:

F(0) = 1 F(1) = 1 F(n) = F(n-1) + F(n-2) Започва се с 0 и 1, а всеки следващ член на редицата се получава като сума на предходните два. Първите няколко числа на Фибоначи са

0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89... Ето някои от основните свойства на числата на Фибоначи:

(F(n),F(m))=F((m,n)) т.е. НОД на числата F(n) и F(m) e число на Фибоначи с индекс НОД(m,n) F(n+k)=F(k-1)*F(n) + F(k)*F(n+1) F(k)/F(kn) за произволно n Отношенията {\displaystyle {\frac {F_{n+1}}{F_{n}}}} {\displaystyle {\frac {F_{n+1}}{F_{n}}}} са приближени дроби на златното сечение ? и по-специално {\displaystyle \lim {n\to \infty }{\frac {F{n+1}}{F_{n}}}=\phi } {\displaystyle \lim {n\to \infty }{\frac {F{n+1}}{F_{n}}}=\phi }. Числата на Фибоначи могат да се бележат и с u(n).

6 Интерполационно търсене

Ин­тер­по­ла­ци­он­но тър­се­не

Ко­га­то чо­век тър­си "Бо­нев" в те­ле­фон­ния ука­за­тел, той гле­да в на­ча­ло­то на ука­за­те­ля, а ко­га­то тър­си "Я­нев" — в края. Не бих­ме ли мо­гли по по­до­бен, на­чин да оп­ти­ми­зи­ра­ме дво­ич­но­то тър­се­не?

Не­ка го раз­гле­да­ме по­-вни­ма­тел­но. При дво­ич­но­то тър­се­не. из­би­рах­ме сред­ния еле­мент mid по фор­му­ла­та mid = (l+r)/2. Да го за­пи­шем по друг на­чин:

mid = l + (r-l)/2 (*)

По­лу­че­на­та фор­му­ла по­каз­ва, че след­ва­ща­та по­зи­ция на раз­де­ля­не на ин­терва­ла се по­лу­ча­ва, ка­то към на­ча­ло­то му до­ба­вим по­ло­ви­на­та на дъл­жи­на­та му. В слу­чая на те­ле­фон­ния ука­за­тел, чо­век не ра­бо­ти по та­зи фор­му­ла. Вмес­то в среда­та той тър­си съ­от­вет­но в на­ча­ло­то или в края на ука­за­те­ля То­ва мо­же да се изра­зи чрез фор­му­ла от ти­па на (*), ка­то за цел­та 1/2 се за­ме­ни с дру­го подхо­дя­що чи­сло. Как да из­бе­рем то­ва чи­сло? Не­ка се вър­нем от­но­во към те­ле­фон­ния ука­за­тел. Там чо­век се опит­ва да опре­де­ли при­бли­зи­тел­на­та по­зи­ция на тър­се­но­то име, из­хож­дай­ки от пър­ва­та му бук­ва, зна­ей­ки пър­ва­та и по­след­на­та бук­ви от аз­бу­ка­та. По ана­ло­ги­чен на­чин бих­ме мо­гли да опи­та­ме да на­ме­рим при­бли­зи­телна­та по­зи­ция на про­из­во­лен еле­мент X, зна­ей­ки стой­но­ст­та X, как­то и стой­нос­тите на на­ча­ло­то и края на раз­глеж­да­ния ин­тер­вал. Яс­но е, че та­зи от­но­си­тел­на по­зи­ция мо­же да се за­да­де с ин­тер­по­ла­ци­он­на­та фор­му­ла:

mid = l + k*(r-l),

7 Шенън-Фано

Методът изисква да са известни вероятностите за срещането на всеки един символ в съобщението. На базата на тези вероятности се конструира таблица със следните характеристики:

всеки символ се представя чрез код

различните кодове имат различен брой битове

кодовете за символи с по-ниска вероятност имат повече битове от тези с по-висока вероятност

кодовете могат еднозначно да бъдат декодирани в символи

Алгоритъмът за конструриарне на таблицата работи по следния начин:

За симовлите се създава таблица с вероятността за тяхното срещане.

Таблицата се сортира според честотата на срещане, като символите с най-голяма вероятност са в началото на списъка.

Таблицата се разделя на две части, като броят на всички срещания на символите в първата й част е максимално близък до броя на всички срещания на символите от втората част.

При генерирането на кода на символите от първата част се присвоява нула, а на тези от втората част – единица.

Алгоритъмът се прилага рекурсивно, докато във всяка група остане само един символ.

8 Aлгоритъм на Хъфман Статистически (вероятностни) методи Основават се на статистически наблюдения за вероятността на поява на буквите (по-рядко групи от букви) във входното съобщение. Идеята е, че ако са известни честотите на срещане за всяка от буквите, бихме могли да ги кодираме с някакъв неравномерен код (най-общо съпоставящ на различните букви кодове с различна дължина), съпоставяйки на най-често срещаните букви по-кратки битови последователности. Типичен представител на този вид кодиране е добре известният Морзов код, съставен от два символа: “?” и “–”. В английския му вариант букви като “е” и “t” се кодират с единствен символ (виж таблица 10.2.), тъй като са сред най-често срещаните. Други важни представители, на които ще обърнем особено внимание, са кодовете на Шенън-Фано (виж 10.4.1.) и Хъфман (виж 10.4.2.), аритметичното кодиране (виж 10.4.5.) и др. Речникови Хъфмановото кодиране съпоставя на всяка буква целочислен брой битове, поради което не дава потенциално оптималния резултат (макар да дава оптимален целочислен код). За съжаление, за да се възползваме по-добре от точните вероятности за срещане на всяка от буквите, е необходимо да можем да им съпоставяме нецелочислен брой битове. Как може да стане това? Един възможен отговор на въпроса дава аритметичното кодиране. Подобно на Хъфмановото кодиране, то също се основава на статистически наблюдения за честотата на срещане на всеки от символите във входното съобщение, но може да постигне значително по-добри резултати, тъй като изобщо се отказва от побуквеното кодиране. Вместо това на цялото входно съобщение се съпоставя едно-единствено дробно число между 0 и 1 с необходимата точност.

Съществуват и други алгоритми: "Разделяй и владей"

Като: Алгоритъмът на Евклид, Алгоритъм от Роберт Ковалски, Симплекс методът, Симулирано закаляване И Т.Н. 